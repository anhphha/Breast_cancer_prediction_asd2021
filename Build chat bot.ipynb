{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/chatbot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "what is breast cancer\n",
      "ROBO: can breast cancer be found early?,\"breast cancer is sometimes found after symptoms appear, but many women with breast cancer have no symptoms.\n",
      "smoking is a risk of breast cancer\n",
      "ROBO: does smoking cause breast cancer?,\"smoking is a confirmed risk factor for many types of cancer.\n"
     ]
    }
   ],
   "source": [
    "#Meet Robo: your friend\n",
    "\n",
    "#import necessary libraries\n",
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "\n",
    "# uncomment the following only the first time\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n",
    "\n",
    "\n",
    "#Reading in the corpus\n",
    "with open('FQA 10 short answers.csv','r', encoding='utf8', errors ='ignore') as fin:\n",
    "    raw = fin.read().lower()\n",
    "\n",
    "#TOkenisation\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "# Preprocessing\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "\n",
    "# Keyword Matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "\n",
    "# Generating response\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n",
    "\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/python-engineer/pytorch-chatbot/blob/master/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/topics/python-chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/charansai612/chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf\n",
      "  Downloading tf-1.0.0.tar.gz (620 bytes)\n",
      "Building wheels for collected packages: tf\n",
      "  Building wheel for tf (setup.py): started\n",
      "  Building wheel for tf (setup.py): finished with status 'done'\n",
      "  Created wheel for tf: filename=tf-1.0.0-py3-none-any.whl size=1284 sha256=65984a16f296a6d488081e94f36b2fbdd1d0aa4f1854d762e0ac2f7d11c6d800\n",
      "  Stored in directory: c:\\users\\phuong\\appdata\\local\\pip\\cache\\wheels\\2c\\6e\\f8\\1e3b5bd4457a9ca1240eb36a4799a0a4a24db275ba5f787a24\n",
      "Successfully built tf\n",
      "Installing collected packages: tf\n",
      "Successfully installed tf-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf #tf Tensorflow environment anaconda navigator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from tflearn) (1.20.2)\n",
      "Requirement already satisfied: six in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from tflearn) (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.60.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Phuong\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "#Used in Tensorflow Model\n",
    "import numpy as np\n",
    "import tensorflow as tf #open and use anaconda navigator\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "#Usde to for Contextualisation and Other NLP Tasks.\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "#Other\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Intents.....\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing the Intents.....\")\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming, Lowering and Removing Duplicates.......\n",
      "31 documents\n",
      "14 classes ['BMI', 'age', 'alcohol', 'diagnosis', 'diseases', 'gene', 'goodbye', 'greeting', 'hormone', 'race', 'radiation', 'recommendation', 'symptoms', 'thanks']\n",
      "66 unique stemmed words [\"'s\", 'a', 'about', 'ag', 'alcohol', 'any', 'anyon', 'ar', 'breast', 'bye', 'cant', 'chang', 'country', 'day', 'diff', 'diseas', 'do', 'doe', 'extern', 'famy', 'from', 'gen', 'good', 'goodby', 'has', 'hav', 'hello', 'help', 'hi', 'high', 'hist', 'hormon', 'how', 'i', 'increas', 'is', 'lat', 'less', 'my', 'nat', 'nee', 'of', 'overweight', 'peopl', 'rady', 'realt', 'recogn', 'recommend', 'rel', 'risk', 'see', 'self-examine', 'should', 'symptom', 'thank', 'that', 'the', 'ther', 'therapy', 'to', 'typ', 'what', 'which', 'worry', 'you', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Data for our Model.....\n",
      "Creating an List (Empty) for Output.....\n",
      "Creating Traning Set, Bag of Words for our Model....\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the Data for our Model.....\")\n",
    "training = []\n",
    "output = []\n",
    "print(\"Creating an List (Empty) for Output.....\")\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n",
      "Creating Train and Test Lists.....\n",
      "Building Neural Network for Out Chatbot to be Contextual....\n",
      "Resetting graph data....\n"
     ]
    }
   ],
   "source": [
    "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "print(\"Creating Train and Test Lists.....\")\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Building Neural Network for Out Chatbot to be Contextual....\")\n",
    "print(\"Resetting graph data....\")\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    }
   ],
   "source": [
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "print(\"Training....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 19999  | total loss: \u001b[1m\u001b[32m0.53486\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5000 | loss: 0.53486 - acc: 0.9078 -- iter: 30/31\n",
      "Training Step: 20000  | total loss: \u001b[1m\u001b[32m0.48813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5000 | loss: 0.48813 - acc: 0.9170 -- iter: 31/31\n",
      "--\n",
      "Saving the Model.......\n",
      "INFO:tensorflow:C:\\Users\\Phuong\\Documents\\Python Scripts\\BDA course\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Model.......\")\n",
    "model.fit(train_x, train_y, n_epoch=5000, batch_size=10, show_metric=True) # test n_epoch 1000, 5000, batch size 8, 10\n",
    "print(\"Saving the Model.......\")\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle is also Saved..........\n"
     ]
    }
   ],
   "source": [
    "print(\"Pickle is also Saved..........\")\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pickle.....\n",
      "Loading the Model......\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Phuong\\Documents\\Python Scripts\\BDA course\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading Pickle.....\")\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    \n",
    "print(\"Loading the Model......\")\n",
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR_THRESHOLD = 0.25\n"
     ]
    }
   ],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # Stemming means to find the root of the word.\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "print(\"ERROR_THRESHOLD = 0.25\")\n",
    "\n",
    "def classify(sentence):\n",
    "    # Prediction or To Get the Posibility or Probability from the Model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # Exclude those results which are Below Threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # Sorting is Done because heigher Confidence Answer comes first.\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # That Means if Classification is Done then Find the Matching Tag.\n",
    "    if results:\n",
    "        # Long Loop to get the Result.\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # Tag Finding\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # Random Response from High Order Probabilities\n",
    "                    return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You- hi\n",
      "Hi there, how can I help?\n",
      "You- what is breast cancer\n",
      "your nipple discharge other than breast milk, including blood\n",
      "You- diagnosis\n",
      "Hello, thanks for visiting\n",
      "You- bye\n",
      "Have a nice day\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b154cd173fb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You- \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0manswer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             )\n\u001b[1;32m--> 848\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    input_data = input(\"You- \")\n",
    "    answer = response(input_data)\n",
    "    answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chatterbot in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: pint>=0.8.1 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (0.17)\n",
      "Requirement already satisfied: mathparse<0.2,>=0.1 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (0.1.2)\n",
      "Requirement already satisfied: sqlalchemy<1.3,>=1.2 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (1.2.19)\n",
      "Requirement already satisfied: pymongo<4.0,>=3.3 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (3.11.3)\n",
      "Requirement already satisfied: python-dateutil<2.8,>=2.7 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (2.7.5)\n",
      "Requirement already satisfied: chatterbot-corpus<1.3,>=1.2 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (1.2.0)\n",
      "Requirement already satisfied: nltk<4.0,>=3.2 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot) (3.6.2)\n",
      "Requirement already satisfied: PyYAML<4.0,>=3.12 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from chatterbot-corpus<1.3,>=1.2->chatterbot) (3.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk<4.0,>=3.2->chatterbot) (4.60.0)\n",
      "Requirement already satisfied: click in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk<4.0,>=3.2->chatterbot) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk<4.0,>=3.2->chatterbot) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk<4.0,>=3.2->chatterbot) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from pint>=0.8.1->chatterbot) (20.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil<2.8,>=2.7->chatterbot) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\phuong\\anaconda3\\envs\\tf\\lib\\site-packages (from packaging->pint>=0.8.1->chatterbot) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install chatterbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytz\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Installing collected packages: pytz\n",
      "Successfully installed pytz-2021.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'time' has no attribute 'clock'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a1bcb1cd3bcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create a new chat bot named Charlie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mchatbot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatBot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Olkkikukat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mListTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchatbot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\chatterbot\\chatterbot.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogic_adapters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_adapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0madapter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogic_adapters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\chatterbot\\utils.py\u001b[0m in \u001b[0;36minitialize_class\u001b[1;34m(data, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\chatterbot\\storage\\sql_storage.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msessionmaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sqlalchemy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# the MIT License: http://www.opensource.org/licenses/mit-license.php\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_util\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minspection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBLANK_SCHEMA\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sqlalchemy\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mupdate_wrapper\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoerce_generator_arg\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolumn_dict\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sqlalchemy\\util\\_collections.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbinary_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools_filterfalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwin32\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mjython\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mtime_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mtime_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'time' has no attribute 'clock'"
     ]
    }
   ],
   "source": [
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ListTrainer\n",
    "\n",
    "# Create a new chat bot named Charlie\n",
    "chatbot = ChatBot('Olkkikukat')\n",
    "\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "trainer.train(['breast cancer','risk','symptoms'])\n",
    "\n",
    "while True:\n",
    "\tinput_data = input(\"You- \")\n",
    "\tresponse = chatbot.get_response(input_data)\n",
    "\tprint(\"Olkkikukat- \",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
